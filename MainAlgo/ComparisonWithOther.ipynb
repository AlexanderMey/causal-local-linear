{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4045583740d541c88ac6a60add5ef413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9448c92fc24935b8a504b22292d3ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Nodes:\n",
      "X1;X2;X3\n",
      "\n",
      "Graph Edges:\n",
      "1. X1 o-o X2\n",
      "2. X2 o-o X3\n",
      "\n",
      "[[ 0 -1  0]\n",
      " [-1  0 -1]\n",
      " [ 0 -1  0]]\n"
     ]
    }
   ],
   "source": [
    "from causallearn.search.ConstraintBased.PC import pc\n",
    "\n",
    "### Changing sample size\n",
    "np.random.seed(1)\n",
    "d=6  #Dimensionality\n",
    "supp=(1,2) #support indices\n",
    "s=len(supp)  #Number of support entries\n",
    "sample=[8,10,12,14,16,18,20]\n",
    "fn=np.zeros((len(sample)))\n",
    "fp=np.zeros((len(sample)))\n",
    "fnpc=np.zeros((len(sample)))\n",
    "fp=np.zeros((len(sample)))\n",
    "runs=100\n",
    "l=1 # Number of intervals we combine for the statistics\n",
    "B=100 # Bootstrap runs\n",
    "I=50      #Number of Intervals \n",
    "a=[list(itertools.combinations(range(d), k)) for k in range(0,d+1)]\n",
    "subsets = [item for sublist in a for item in sublist]\n",
    "dic={}\n",
    "# We first fix the data for all runs, to eliminate the randomness of this.\n",
    "betas=np.zeros((runs,I,d))\n",
    "for r in range(runs):\n",
    "    for i in range(I):\n",
    "        betas[r,i,supp]=np.random.uniform(low=1,high=5,size=(s))\n",
    "\n",
    "for o,n in enumerate(sample):\n",
    "    print('Sample size',o+1,'of',len(sample))\n",
    "    for r in range(runs):\n",
    "        X=[]\n",
    "        Y=[]\n",
    "        for i in range(I):\n",
    "            x=np.zeros((n,d))\n",
    "            y=np.zeros((n))\n",
    "            std=np.random.uniform(low=1,high=5,size=(d))\n",
    "            x[:,0]=np.random.normal(scale=std[0],size=(n))\n",
    "            x[:,1]=x[:,0]+np.random.normal(scale=std[1],size=(n))\n",
    "            x[:,2]=0.3*x[:,1]+np.random.normal(scale=std[2],size=(n))\n",
    "            x[:,3]=0.2*x[:,2]+np.random.normal(scale=std[3],size=(n))\n",
    "            e=np.random.multivariate_normal(0*np.ones(n),1*np.eye(n))\n",
    "            y=x@betas[r,i,:]+e\n",
    "            x[:,4]=np.random.normal(scale=std[4],size=(n))+0.1*x[:,1]+y\n",
    "            x[:,5]=np.random.normal(scale=std[5],size=(n))+y\n",
    "            x_pc[i*n:(i+1)*n,:6]=copy.copy(x)\n",
    "            x_pc[i*n:(i+1)*n,:6]=copy.copy(y)\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "\n",
    "            \n",
    "        plausibleS=loli.gauss(X,Y)\n",
    "        if not not plausibleS:\n",
    "            supphat=set.intersection(*plausibleS)\n",
    "            if len(supphat.difference(set(supp)))>0:\n",
    "                fp[o]+=1/runs\n",
    "            if len(set(supp).difference(supphat))>0:\n",
    "                fn[o]+=1/runs\n",
    "                \n",
    "        cg = pc(x_pc,alpha=0.1)\n",
    "                \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "\tlabel.set_fontsize(14)\n",
    "plt.plot(sample,0.1*np.ones((len(sample))),label='Confidence level',color='black')\n",
    "plt.plot(sample,fp,label='False Positive SEM',linestyle=(0, (1, 5)))\n",
    "plt.plot(sample,fn,label='False Negative SEM',linestyle=(0, (1, 1)))\n",
    "plt.plot(sample,fpind,label='False Positive GAUSS',linestyle=(0, (5, 5)))\n",
    "plt.plot(sample,fnind,label='False Negative GAUSS',linestyle=(0, (5, 1)))\n",
    "\n",
    "plt.xlabel('Sample Size Per Environment',fontsize=14)\n",
    "plt.ylabel('FP/FN Rate',fontsize=14)\n",
    "plt.legend()\n",
    "# plt.savefig('Figures/Samplechangen.eps',bbox_inches=\"tight\",format='eps')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce5c481bb0724895b81e39fe685daa9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd29d1fdece34917832347e2e27b4b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X7 --> X5\n",
      "[[0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n",
      "The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LassoLarsIC())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "Set parameter alpha to: original_alpha * np.sqrt(n_samples). \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from causallearn.search.FCMBased import lingam\n",
    "supp=(1,2)\n",
    "s=len(supp)\n",
    "d=7\n",
    "n=500 # Sample size\n",
    "I=1      #Number of Intervals \n",
    "# We first fix the data for all runs, to eliminate the randomness of this.\n",
    "betas=np.zeros((I,d))\n",
    "\n",
    "\n",
    "\n",
    "x=np.zeros((I*n,d))\n",
    "for i in range(I):\n",
    "    betas[i,supp]=np.random.uniform(low=1,high=1,size=(s))\n",
    "    std=np.random.uniform(low=1,high=1,size=(d))\n",
    "    x[i*n:(i+1)*n,0]=np.random.uniform(low=-std[0],high=std[0],size=(n))\n",
    "    x[i*n:(i+1)*n,1]=x[i*n:(i+1)*n,0]+np.random.uniform(low=-std[1],high=std[1],size=(n))\n",
    "#     x[i*n:(i+1)*n,0]=np.random.normal(scale=10*std[0],size=(n))\n",
    "#     x[i*n:(i+1)*n,1]=x[i*n:(i+1)*n,0]+np.random.normal(scale=std[1],size=(n))\n",
    "    x[i*n:(i+1)*n,2]=0.3*x[i*n:(i+1)*n,1]+np.random.uniform(low=-std[1],high=std[1],size=(n))\n",
    "    x[i*n:(i+1)*n,3]=0.2*x[i*n:(i+1)*n,2]+np.random.uniform(low=-std[1],high=std[1],size=(n))\n",
    "    e=np.random.uniform(low=-std[1],high=std[1],size=(n))\n",
    "#     e=np.random.uniform(low=-1,high=1,size=(n))\n",
    "    x[i*n:(i+1)*n,6]=x[i*n:(i+1)*n,:]@betas[i,:]+e\n",
    "    x[i*n:(i+1)*n,5]=np.random.uniform(low=-std[1],high=std[1],size=(n))\n",
    "    x[i*n:(i+1)*n,4]=np.random.uniform(low=-std[1],high=std[1],size=(n))+0.1*x[i*n:(i+1)*n,1]+x[i*n:(i+1)*n,6]+x[i*n:(i+1)*n,5]\n",
    "data=x\n",
    "# default parameters\n",
    "cg = pc(data,alpha=0.1)\n",
    "G, edges = fci(data)\n",
    "\n",
    "# print(G)\n",
    "# print(cg.G.graph)\n",
    "\n",
    "model = lingam.ICALiNGAM()\n",
    "model.fit(data)\n",
    "# print(model.causal_order_)\n",
    "print(np.round(model.adjacency_matrix_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
